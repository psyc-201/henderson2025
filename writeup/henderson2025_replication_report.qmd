---
title: "Reproducibility of Henderson et al. (2025, Nature Communications)"
author: "Lena L. Kemmelmeier (lkemmelmeier@ucsd.edu)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
format:
  html:
    toc: true
    toc_depth: 10
    format:
    self-contained: true
---

## Links  
- [GitHub Repo](https://github.com/Lena-Kemmelmeier/henderson2025)  
- [Henderson et al. (2025) Paper (PDF)](https://github.com/Lena-Kemmelmeier/henderson2025/blob/main/original_paper/henderson_2025.pdf){target=_blank}  
- [Link to Pre-registration (DOI)](https://doi.org/10.17605/OSF.IO/F75RS)  

## Introduction

**Background: Original Study**  
Henderson et al. (2025) investigated the neural mechanisms of dynamic categorization, specifically how visual cortex adapts its responses to shape stimuli under different decision-rule (categorization) contexts. Each participant categorized novel stimuli along two linear (Linear-1, Linear-2) boundaries and a Nonlinear boundary across three fMRI sessions. Behavioral performance (accuracy and reaction time) and decoding of voxel activation patterns suggested that sensory shape representations adapted to support the discrimination of the *currently* relevant categories, particularly when categorization was most difficult (i.e., when shapes were near, rather than far from, the decision boundary). Altogether, these findings suggest that early sensory areas may contribute to flexible decision making.

**Reproducibility & Simulation Aims**  
In the original study, the authors trained classifiers to predict category membership using voxel activation patterns (one for each boundary condition). They found that classifier accuracy depended on the current task (i.e., which boundary participants were using at that moment). For example, a three-way ANOVA including ROI, Task, and Boundary showed that when participants performed the Linear-2 task, the Linear-2 classifier was better at predicting a shape’s category than the Linear-1 classifier, specifically for shapes near the decision boundary (Task × Boundary: *F*(1,9) = 8.99, *p* = .011, ηₚ² = 0.50). The authors interpreted this as evidence that shape representations adapt to become more separable across the *currently* relevant boundary. I aimed to reproduce this Task × Boundary interaction. I also attempted to reproduce their broad pattern of classifier accuracies: decoding accuracies should be highest in early visual ROIs (V1, V2, V3) and lowest in later visual ROIs (e.g., IPS), and accuracy for the Nonlinear classifier should be lower than for the Linear-1 and Linear-2 classifiers. Separately, I created a new sample of novel shape stimuli that varied continuously across two features (contours) and examined whether they yielded the same key properties of the original set. Specifically, I checked that these new stimuli roughly conformed to a two-dimensional space after dimensionality reduction, and whether stimuli were most perceotually separable across the Linear-2 and Linear-1 categories compared to the Nonlinear category, as in the original study.

## Methods

### Reliability and Validity
The original authors did not include any reliability metrics for their data.

### Power Analysis
A post-hoc power analysis showed that the current sample (N = 10) provided 76% power to detect the observed two-way interaction (ηₚ² = .50, Cohen’s f = 0.999) at α = .05. To achieve 80%, 90% and 95% power, the original study would have needed to collect sample sizes of approximately 11, 14, and 16 participants, respectively.

### Sample
In the original study, 10 participants (7 female) underwent three fMRI sessions, during which they completed a shape categorization task. Participants were recruited from the UCSD community, and were between 24 and 33 (M: 28.2, SD: 3.0) years old. All participants met inclusion criteria of having normal or corrected-to-normal vision.

### Materials

**Shape Stimuli**
I closely followed the original protocol when it came to constructing my shape stimuli. However, in order to create a different stimulus set, I varied a different pair of radial frequency components (RFCs, contours) than those used in the original study. Each RFC defined a sinusoidal modulation of a base circle’s radius as a function of angle, and varying its amplitude changed the prominence of that modulation in the contour. As in the original design, the amplitudes of two RFCs was systematically varied while the remaining RFCs were held constant. These two amplitude values defined the x and y coordinates of a continuous two-dimensional shape space, where each coordinate pair represented to a unique shape.

The manipulation of RFC amplitude defined an x/y grid that spans positions between 0 and 5 arbitrary units, representing relative changes in modulation strength. In contrast to the original 4x4 grid of 16 stimuli with coordinates [0.1, 1.7, 3.3, 4.9], I instead generated a 6x6 grid of 36 stimuli covering the same range. Grid positions were evenly spaced and slightly inset from the edges ([0.1, 1.06, 2.02, 2.98, 3.94, 4.9]). This denser grid sampled from the shape space led to smoother transitions between neighboring shapes. The overall coordinate system (0–5 range with a midpoint at 2.5) was the same.

***Shape Protocol of Henderson et al.*** "We used a set of shape silhouette stimuli that varied parametrically along two continuous dimensions, generating a 2-dimensional shape space (Fig. 1A). Each shape in this space was a closed contour composed of RFCs29,30. Each shape was composed of 7 different RFCs, where each component has a frequency, amplitude, and phase. We selected these stimuli because they can be represented in a low-dimensional grid-like coordinate system, but are more complex and abstract relative to simpler stimuli such as oriented gratings. Importantly, the changes along each axis in the shape space involve variability in multiple regions of the image, so categorizing the shapes correctly required participants to integrate information globally across the image, rather than focusing on a single part of the shape. For example, to categorize the shapes along axis 1 (Fig. 1A), it might be necessary to integrate information about both the size of the top left lobe, and the shape of the protrusion on the right side. Thus, it would not be possible to categorize the shapes by attending to one focal spatial location only.

To generate the 2-dimensional shape space, we parametrically varied the amplitude of two RFCs, leaving the others constant. The manipulation of RFC amplitude was used to define an x/y grid in arbitrary units that spanned positions between 0 and 5 arb. units, with adjacent grid positions spaced by 0.1 arb. units. All shape space positions on all trials were sampled from this grid of shape space positions. We also defined a coarser grid of 16 points (a 4 × 4 grid) which was used to generate the 16 stimuli that were shown on the majority of trials; this grid is referred to as the “main grid”, and included all x/y combinations of the points [0.1, 1.7, 3.3, 4.9] in shape space coordinates. Stimuli corresponding to points in shape space that were not part of the main grid were used to make the tasks more difficult, see Main task design for details.

We divided the shape space into four quadrants by imposing boundaries at the center position of the grid (2.5 arb. units) in each dimension. To define the binary categories that were relevant for each task (see Main task design), we grouped together two quadrants at a time, with the Linear-1 task and Linear-2 tasks grouping quadrants that were adjacent (creating either a vertical or horizontal linear boundary in shape space), and the Nonlinear task grouping quadrants that were non-adjacent (creating a Nonlinear boundary). During task training as well as before each scanning run, we utilized a “prototype” image for each shape space quadrant as a way of reminding participants of the current categorization rule. The prototype for each quadrant was positioned directly in the middle of the four main grid positions corresponding to that quadrant (i.e., the x/y coordinates for the prototypes were combinations of [0.9, 4.1] arb. units). These prototype images were never shown during the categorization task trials, to prevent participants from simply memorizing the prototypes. Shapes used in the task were also never positioned exactly on any quadrant boundary in order to prevent any ambiguity about category."


### Procedure	
This reproducibility and simulation project did not involve aquisition of additioanal behavioral data, or of any of the task-based fMRI data.

***Main Task Procedure of Henderson et al.***  "The main experimental task consisted of categorizing shape silhouette stimuli (Fig. 1) into binary categories. There were three task conditions: Linear-1, Linear-2, and Nonlinear, each of which corresponded to a different binary categorization rule. Shape stimuli were drawn from a two-dimensional shape space coordinate system (see Shape stimuli). The Linear-1 and Linear-2 tasks used a boundary that was linear in this shape space, while the Nonlinear task used a boundary that was Nonlinear in this shape space (requiring participants to group non-adjacent quadrants into a single category, see Fig. 1 for illustration). Each trial consisted of the presentation of one shape for 1 s, and trials were separated by an inter-trial interval (ITI) that was variable in length, uniformly sampled from the interval 1–5 s. Participants responded on each trial with a button press (right index or middle finger) to indicate which binary category the currently viewed shape fell into; the mapping between category and response was counter-balanced within each scanning session. Participants were allowed to make a response anytime within the window of 2 s from stimulus onset. Feedback was given at the end of each run, and included the participant’s overall accuracy, as well as their accuracy broken down into “easy” and “hard” trials (see next paragraph for description of hard trials), and the number of trials on which they failed to respond. No feedback was given after individual trials.

Each run in the task consisted of 48 trials and lasted 261 s (327 TRs). Of the 48 trials, 32 of these used shapes that were sampled from a grid of 16 points evenly spaced within shape space (“main grid”, see Shape stimuli), each repeated twice. These 16 shapes were presented twice per run regardless of task condition. The remaining 16 trials (referred to as “hard” trials) used shapes that were variable depending on the current task condition and the difficulty level set by the experimenter. The purpose of these trials was to allow the difficulty level to be controlled by the experimenter so that task accuracy could be equalized across all task conditions, and prevent any single task from being trivially easy for each participant. For each run of each task, the experimenter selected a difficulty level between 1 and 13, with each level corresponding to a particular bin of distances from the active categorization boundary (higher difficulty denotes closer distance to boundary). These difficulty levels were adjusted on each run during the session by the experimenter, based on performance on the previous run, with the goal of keeping the participant accuracy values within a stable range for all tasks (target range was around 80% accuracy). For the Nonlinear task, the distance was computed as a linear distance to the nearest boundary. The “hard” trials were generated by randomly sampling 16 shapes from the specified distance bin, with the constraint that 4 of the shapes had to come from each of the four quadrants in shape space. This manipulation ensured that responses were balanced across categories within each run. For many of the analyses presented here, we excluded these hard trials, focusing only on the “main grid” trials where the same images were shown across all task conditions.

Participants performed 12 runs of the main task within each scanning session, for a total of 36 runs across all 3 sessions (with the exception of one participant (S06) for whom 3 runs are missing due to a technical error). The 12 runs in each session were divided into 6 total “parts” where each part consisted of a pair of 2 runs having the same task condition and the same response mapping (3 conditions × 2 response mappings = 6 parts). Each part was preceded by a short training run, which consisted of 5 trials, each trial consisting of a shape drawn from the main grid. The scanner was not on during these training runs, and the purpose of these was to remind the participant of both the currently active task and the response mapping before they began performing the task runs for that part. The order in which the 6 parts were shown was counter-balanced across sessions. Before each scan run began, the participant was again reminded of the current task and response mapping via a display that presented four prototype shapes, one for each shape space quadrant (see Shape stimuli for details on prototype shapes). The prototypes were arranged with two to the left of fixation and two to the right of fixation, and the participant was instructed that the two leftmost shapes corresponded to the index finger button and the two rightmost shapes corresponded to the middle finger button. This display of prototype shapes was also used during the training runs to provide feedback after each trial: after each training trial, the four prototype shapes were shown, and the two prototypes corresponding to the correct category were outlined in green, with accompanying text that indicated whether the participant’s response was correct or incorrect. This feedback display was not shown during the actual task runs.

Before the scan sessions began, participants were trained to perform the shape categorization tasks in a separate behavioral session (training session took place on average 4.0 days before the first scan session). During this behavioral training session, participants performed the same task that they performed in the scanner, including 12 main task runs (2 runs for each combination of condition and response mapping; i.e., each of the 6 parts). As in the scan sessions, each part was preceded by training runs that consisted of 5 trials, each accompanied by feedback. Participants completed between 1 and 3 training runs before starting each part. Average training session accuracy was 0.81 ± 0.02 (mean ± SEM across 10 participants) for the Linear-1 task, 0.81 ± 0.02 for the Linear-2 task, and 0.78 ± 0.02 for the Nonlinear task."


### Analysis Plan

**Binary Decoding of Voxel Activation Data (Reproducibility)**  

***I tested whether I could reproduce the reported Task × Boundary interaction effect on binary decoding accuracy from the ANOVA that included ROI, Task, and Boundary on voxel activation data for near (what the original authors call 'hard') trials, and whether I could reproduce the overall accuracies of the three decoders corresponding to each task boundary condition across ROIs.***

My decoding approach largely mirrored that of the original study: for each participant, ROI, and task, I trained separate L2-regularized logistic-regression classifiers (with the lbfgs solver) using the voxel activation pattern from each individual trial as the input feature vector. In other words, every trial contributed a voxel pattern for each ROI, and the classifier learned to map these patterns to one of the two categories (either defined by the Linear-1, Linear-2, or Nonlinear boundaries). I used the same 20 log-spaced C values (ranging from 1e-9 to 1e1) for regularization and selected the optimal C using nested cross-validation within the training data, with leave-one-run-out cross-validation at the outer level. To make sure there was stable convergence across ROIs, I increased the maximum iteration limit to 20,000 and also relaxed the convergence tolerance (tol = 1e-3). I implemented all steps for this supervised classification pipeline in Python using the sci-kit learn library (version 1.0.2, this matched the version used by the original authors).

Because my goal was to reproduce the Task × Boundary interaction, I used these classifier accuracies directly as the dependent variable in the three-way repeated-measures ANOVA (included ROI, Task, and Boundary). Following Henderson et al. (2025), I binned decoding accuracy separately for the  near and far trials based on each stimulus’s boundary distance, and I restricted the ANOVA to near-trial accuracies for the two linear tasks and two linear boundaries, which is the subset that produced the Task × Boundary interaction in the published study. 

As a visual check, I also plotted the overall decoding accuracy patterns (Figure 2A-C of the original paper) to confirm that my pattern of results matched those of Henderson et al. (2025) for the classifier predicting Linear-1 category, classifier predicting Linear-2 category, and classifier predicting Nonlinear category.

***Key Reproducibility Criteria*** I defined reproducibility success for the above-mentioned Task × Boundary effect as meeting the threshold for frequentist significance (p-value less than .05) and obtaining an effect size similar to the original study (ηₚ² = .50). Here, the effect size was considered of similar size if it fell within plus or minus 0.10 of the original effect size (ηₚ² = .50). 

For the broader decoding accuracy values displayed in the visual plots (my replication of the authors’ Figure 2A–C), I defined reproducibility success as the means deviating of, at most, about 10 percent across ROIs and tasks (compared to the original study), with all means of decoding accuracies remaining above chance across all ROIs and the three boundary classifiers. Moreover, in a successful reproduction, early visual areas (V1, V2) should show the highest decoding accuracy and later areas (LO1, LO2, IPS) should show lower but still above-chance accuracy. Across ROIs, accuracy should follow the pattern such that decoding accuracies are generally highest for the Linear-2 classifier, then Linear-1, and lowest for the Nonlinear classifier. This will be determined based on side-by-side comparison of my plots and the authors' original sub-figures.

**Image Similarity & Feature Space Analyses (Simulation)**

***I tested whether my newly generated shape stimuli (see Shape Stimuli section) yielded similar properties to the stimuli used in the original study. Specifically, I checked 1) How perceptually separable were items in each category across the three task boundary conditions? and 2) Did my stimulus set conform to a two-dimensional space?***

For my simulation portion of this project, I replicated the author's original protocol exactly such that I used pre-trained versions of the  SimCLR model (available for download) and GIST model using the scripts available in the authors’ repo. Further, I applied principal component analysis (PCA) on the GIST feature vectors to visually examine whether my stimuli roughly conformed to a two-dimensional space. This meant that I plotted PC1 against PC2 and colored the shapes according to their coordinates on the x- (shape axis 1) and also coordinates on the y-axis (shape axis 2).

***Image Similarity/Category Separability & PCA Procedure of Henderson et al.*** "To estimate the perceptual discriminability of our shape categories, we used two computer vision models to extract activations in response to each stimulus image. We first used the GIST model31, which is based on Gabor filters and captures low-level spectral image properties. We also extracted features from a pre-trained SimCLR model32, which is a self-supervised model trained using contrastive learning on a large image database. We selected these two models because the GIST model captures clearly defined image properties similar to those represented in the early visual system, while the SimCLR model can capture a wider set of image features, including mid-level and high-level properties. The GIST model was implemented in Matlab, using a 4 × 4 spatial grid, 4 spatial scales, and 4 orientations per spatial scale. The version of SimCLR that we used was implemented in PyTorch and used a ResNet-50 backbone (pre-trained model downloaded from https://pypi.org/project/simclr/). We extracted activations from blocks 2, 6, 12, and 15, and performed a max-pooling operation (kernel size = 4, stride = 4) to reduce the size of activations from each block. We used principal components analysis (PCA) to further reduce the size of activations, retaining a maximum of 500 components per block, and concatenated the resulting features across all blocks.

Using these activations, we computed the separability of shape categories across each of our boundaries (Linear-1, Linear-2, Nonlinear) by computing all pairwise Euclidean distances between main grid shapes in the same category (within-category distances) and main grid shapes in different categories (between-category distances). We then computed the average of the within-category distances (w) and between-category distances (b). The separability measure for each boundary was computed as: (b−w)/(b+w)."

"To verify the two-dimensional structure of our shape space, we used an image similarity analysis based on GIST features31 (see Methods) to assess the perceptual similarity between shape stimuli. As expected, a principal components analysis (PCA) performed on the GIST features revealed a two-dimensional grid structure, with the two shape space axes oriented roughly orthogonal to one another in PC space (Fig. 1C). "

***Key Assessment Criteria*** 
In a successful extension, I expected my images to be most separable across the Linear-1 and Linear-2 boundaries and least separable across the Nonlinear boundary according to the GIST and SimCLR activations. In line with the authors' results, I expected a successful extension to show higher category separability for GIST than for SimCLR. For the PCA, I expected a clear two-dimensional grid in PC space reflecting the two RFC dimensions. Specifically, I expected that when I visualized the feature space separately for each shape dimension, the shape dimensions would resemble the original authors' Figure 1C, such that the two shape space axes appeared roughly orthogonal to each other when plotted against PC1 and PC2 and when the shape representations were colored according to their coordinates along each axis.

### Differences from Original Study
I used the preprocessed voxel activation data made availible by the original authors instead of reanalyzing the raw fMRI data, and I only reproduced the decoding and ANOVA portions rather than the full fMRI and behavioral acquisition. I also omitted any behavioral analysis. Moreover, I re-implemented the binary decoding and ANOVA pipeline myself rather than using the authors' scripts. I did my best to parallel their classification and cross-validation pipeline, but I did relax the tolerance in the cross-validation (increasing the tol parameter from 1e-4, the default value, to 1e-3) so that the solver stops when the mean change in loss is below 1e-3 rather than 1e-4. I also increased the number of maximum iterations (max_iter) the lbfgs solver is allowed to take from 1,000 to 20,000. I adjusted these values for tol and max_iter after seeing that running with the authors' original parameters (i.e., max_iter = 1,000, tol = 1e-4) led to run-time convergence warnings. Idid not anticipate that these changes would meaningfully affect my reproducibility success, especially given that my statistical effect of interest was large.

For the simulation/extension aim, I generated a new set of two-dimensional shape stimuli that followed the same general structure as the original ones but varied a different pair of radial frequency components. Moreover, instead of the original 4x4 grid, I used a denser 6x6 grid. This decision to create more stimuli in a denser grid was chosen somewhat arbitrarily as a training experience to differentiate my extension from the original. The goal was to see whether I could successfully create novel shapes that exhibited the expected properties, which allowed me to gain experience constructing and evaluating visual stimuli using computer vision models and also subsequent PCA. I did not anticipate that these changes in the extension would affect my ability to create stimuli with the intended structure or interfere with my ability to examine the properties of the stimulus set.

## Results

### Data preparation
Data preparation following the analysis plan. [Just make a note that these occured in another script...]

### Confirmatory analysis
**Binary Decoding of Voxel Activation Data (Reproducibility)**  
The three-way ANOVA including ROI, Task, and Boundary showed that when participants performed the Linear-2 task, the Linear-2 classifier was better at predicting a shape’s category than the Linear-1 classifier, in the context of shapes near the decision boundary (Task × Boundary: *F*(1,9) = 8.17, *p* = .019, ηₚ² = 0.48; **Figure 1**). In other words, I successfully reproduced this key statistic from the original study, as this Task × Boundary effect was very close in size to the originally reported large effect size, ηₚ² = 0.50, and also reached the threshold for significance. 

![Three-way ANOVA that included ROI, Task, and Boundary on voxel activation data for items near the decision boundary.](../bold_decoding_anova_results/figs/Table1_LINEAR_NEARONLY_replica_APA.png){#fig-blobgrid fig-cap="" fig-alt="Three-way ANOVA that included ROI, Task, and Boundary on voxel activation data for items near the decision boundary for the reproducibility attempt." width="70%" fig-align="center"}

Moreover, I successfully reproduced the broader pattern of decoder accuracies. Across all three classifiers, decoding accuracies were generally highest in early visual ROIs (V1, V2, V3) and lowest in later visual ROIs (e.g., IPS), which is consistent with the authors’ point that these were simple shapes whose size and location stayed constant across trials, and thus early visual cortex (V1–V3) was likely sufficient to represent them. Decoding accuracies for the Nonlinear classifier were also lower than for the Linear-1 and Linear-2 classifiers (**Figures 2–4**). Lastly, all mean decoding accuracies remained above chance across all ROIs and classifiers, in line with the original results.

<!-- 
These plots below were created in adobe illustrator --- otherwise I would have plotted and rendered them here! Just wanted to show a nice side by side of the authors' and my results.
!-->
![Binary classifier predicting Linear-1 category. Panel A shows the original results reported in Henderson et al. (2025). Gray dots represent accuracy values for individual participants, and colored circles with error bars represent the mean ± SEM across participants. Panel B shows my reproducibility attempt using the same decoding approach on the shared preprocessed voxel activation data. The horizontal line marks chance-level decoding accuracy. Dark blue indicates decoding performance for the ‘Linear-1’ task, light blue for the ‘Linear-2’ task, and green for the ‘Nonlinear’ task.](../writeup/figs/linear1_results_side_by_side){#fig-2A fig-alt="Binary classifier predicting Linear-1 category. Gray dots show participant accuracy values, and colored circles with error bars show the mean ± SEM. A horizontal line marks chance-level accuracy. Dark blue corresponds to the 'Linear-1' task, light blue to the 'Linear-2' task, and green to the 'Nonlinear' task." width="100%" fig-align="center"}

![Binary classifier predicting Linear-2 category. Panel A shows the original results reported in Henderson et al. (2025), with gray dots representing accuracy values for individual participants and colored circles with error bars indicating the mean ± SEM. Panel B shows my reproducibility attempt using the same decoding pipeline on the authors’ shared voxel activation data. The horizontal line marks chance-level decoding accuracy. Dark blue indicates decoding performance for the ‘Linear-1’ task, light blue for the ‘Linear-2’ task, and green for the ‘Nonlinear’ task.](../writeup/figs/linear2_results_side_by_side){#fig-2B fig-alt="Binary classifier predicting Linear-2 category. Gray dots show participant accuracy values, and colored circles with error bars show the mean ± SEM. A horizontal line marks chance-level accuracy. Dark blue corresponds to the 'Linear-1' task, light blue to the 'Linear-2' task, and green to the 'Nonlinear' task." width="100%" fig-align="center"}

![Binary classifier predicting Nonlinear category. Gray dots represent accuracy values for individual participants, and colored circles with error bars represent the mean ± SEM across 10 participants. The horizontal line marks chance-level decoding accuracy. Dark blue indicates decoding performance for the 'Linear-1' task, light blue for the 'Linear-2' task, and green for the 'Nonlinear' task.](../writeup/figs/nonlinear_results_side_by_side){#fig-2C fig-alt="Binary classifier predicting Nonlinear category. Gray dots show participant accuracy values, and colored circles with error bars show the mean ± SEM. A horizontal line marks chance-level accuracy. Dark blue corresponds to the 'Linear-1' task, light blue to the 'Linear-2' task, and green to the 'Nonlinear' task." width="100%" fig-align="center"}


**Image Similarity & Feature Space Analyses (Simulation)**  
As described in the Shape Stimuli section, I created a grid of 36 novel stimuli through systematically manipulating a different pair of contours from those used in the original study (**Figure 5**). 

![Grids of shape silhouette stimuli. Panel A shows the original authors’ 4×4 shape grid defined by two RFC shape axes, and Panel B shows my extended 6×6 grid created by varying a different set of RFc in a denser manner. Each position in the grid corresponds to a unique combination of amplitudes along shape axis 1 (horizontal) and shape axis 2 (vertical).](../writeup/figs/shape_grids_side_by_side.png){#fig-blobgrid 
fig-alt="Side-by-side image of two shape grids defined by RFC components. Panel A shows the original authors’ 4×4 grid arranged along two shape axes, and Panel B shows my 6×6 extension spanning a denser set of values across the same axes." 
width="80%" fig-align="center"}

From there, I extracted feature vectors using two computer vision models: GIST, which captures lower-level image properties, and SimCLR, which captures more mid- to high-level properties. Category separability, quantified as the ratio of between-category to within-category distances, showed the same overall pattern as the original data. Separability was generally higher for GIST than for SimCLR, and for both models the Linear-1 and Linear-2 boundaries were more separable than the Nonlinear boundary (**Figure 6**). The feature spaces treated the Linear-1 and Linear-2 categories as more visually distinct sets of shapes, while the Nonlinear shape categories were harder to tell apart.

![Category separability for GIST and SimCLR feature spaces. Panel A shows the original authors’ results, and Panel B shows my reproducibility attempt using the same analysis procedures. Each bar quantifies the separability of the shape categories within each feature space, computed as the ratio of between-category to within-category Euclidean distances for the 36 main grid images. Higher values indicate greater category separability. Bar colors correspond to the three category boundaries: dark blue for Linear-1, light blue for Linear-2, and green for the Nonlinear boundary.](../writeup/figs/category_sep_side_by_side.png){#fig-sep 
fig-alt="Side-by-side comparison of category separability for GIST and SimCLR feature spaces. Panel A shows the original authors’ results, and Panel B shows my reproducibility attempt. Each bar shows the ratio of between-category to within-category Euclidean distances. Bar colors correspond to Linear-1 (dark blue), Linear-2 (light blue), and Nonlinear (green)."
width="100%" fig-align="center"}

The PCA results showed that my new stimulus set formed a clear two-dimensional space, and the two shape axes appeared approximately orthogonal to one another, confirming that the intended geometric structure of the stimulus grid was captured (**Figure 7**).

![PCA visualization of GIST feature space for the shape stimuli. Panel A shows the original authors’ results, and Panel B shows my reproducibility attempt. Each point represents one shape projected into the first two principal components (PC1 and PC2). The left plots are colored according to each shape’s coordinate along shape axis 1, and the right plots are colored according to each shape’s coordinate along shape axis 2.](../writeup/figs/gist_pca_side_by_side.png){#fig-pca 
fig-alt="PCA visualization of GIST feature space for the shape stimuli. Panel A shows the original authors’ PCA results and Panel B shows my reproduction. Each point corresponds to a shape plotted in PC1–PC2 space, with color indicating its value along shape axis 1 (left) or shape axis 2 (right)." 
width="60%" fig-align="center"}

## Discussion

### Summary of Replication Attempt (Interpretation & Conclusion)

Open the discussion section with a paragraph summarizing the primary result from the confirmatory analysis and the assessment of whether it replicated, partially replicated, or failed to replicate the original result.  

### Commentary

Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis, (b) assessment of the meaning of the replication (or not) - e.g., for a failure to replicate, are the differences between original and present study ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the replication attempt.  None of these need to be long.

**1. What how does the decoding framework offer insight about neural representations?**


**2. Walk through the main stages of your decoding pipeline and explain how a simulated or simplified dataset would move through each stage. What does this reveal about the assumptions built into your analysis?**


**3. Identify one insight your simulation gave you about the strengths or limitations of the original experimental design.** Running my 6×6 RFC grid through GIST and SimCLR made me appreciate how carefully the original stimulus space was put together. Once I generated my own set and looked at the feature distances, it was obvious that the two axes vary smoothly and predictably, and that the different category boundaries carve up the space in meaningful ways rather than by accident. This helped me see why the authors’ design works so well. The nonlinear boundary in particular cuts across parts of the space that are not naturally separated in visual feature space, which means that any neural differences they report there are harder to explain as simple perceptual effects. Overall, the simulation made the rigor of the stimulus construction much more apparent and gave me a better sense of how intentionally the task and stimuli were aligned.

**4. How would this simulation help you design a follow-up experiment with a similar paradigm?** Doing both the reproduction and simulation parts of this project was useful training for me. Recreating the decoding pipeline helped me understand the structure of the fMRI data at the voxel level and how much the analysis depends on organizing runs, ROIs, and trial labels cleanly. It also gave me more hands-on practice with supervised classification, which I have done before in Python but was still a helpful refresher. On the simulation side, generating my own RFC grid and running it through GIST and SimCLR helped me get more familiar with these computer vision models, even if I was mainly downloading and running the pre-trained versions. Working through that made it easier to see how different manipulations show up in feature space and how the category boundaries relate to those differences. Combining these pieces made the overall workflow feel more transparent. If I were to design a follow-up experiment, I would feel more prepared to choose which RFCs to vary, how dense the stimulus grid should be, and how to structure the data so that later decoding is straightforward.