---
title: "Replication of Henderson et al. (2025, Nature Communications)"
author: "Lena L. Kemmelmeier (lkemmelmeier@ucsd.edu)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
format:
  html:
    toc: true
    toc_depth: 4
    format:
    self-contained: true
---

<!-- Replication reports should all use this template to standardize reporting across projects.  These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

## Introduction

Henderson et al. (2025) investigated the neural mechanisms of perceptual categorization. Specifically, they examined how representations of shape stimuli in the visual cortex change under different decision rule contexts. Participants categorized novel stimuli along two linear category boundaries and one non-linear category boundary during fMRI sessions. Behavioral (accuracy and reaction time) data and decoding of voxel activation data suggested that sensory shape representations adapted to support the discrimination of relevant categories, with there being more prominent effects for shapes close to the decision boundary.

**Justification**  
I will simulate data, rather than conduct an experiment, as my research does not lend itself to data collection this quarter. The research questions of Henderson et al. (2025) do not directly overlap with my first-year project; however, I chose to work with this study because it was led by former lab members and offers a unique opportunity to get familiar with pipelines and techniques that I may use in future projects, and which are broadly applicable to my research in human visual cognition. In attempting to reproduce a statistical result based on the decoding of voxel activation patterns (crucial to the paper’s main claim), I can gain familiarity with classification pipelines. I recognize that simulating fMRI data may fall a bit beyond the scope of this quarter, so instead, I plan to gain experience generating a new stimulus set and working with vision models like SimCLR and GIST in order to check the structure of the stimuli and their separability across boundaries. Taking these separate routes for the reproducibility check and simulation portions of the project will allow me to gain proficiency in different techniques valuable to my training in the Serences Lab.

**Analyses/Techniques**  

***Reproducing Statistical Result: Binary Classifier***  
I plan to assess whether I can reproduce the reported Task × Boundary interaction effect on decoding accuracy from the ANOVA that included ROI, Task, and Boundary on voxel activation data. The authors used this significant interaction to support the claim that the structure of category representations (which boundary best separates neural patterns) changes depending on the task context. The preprocessed voxel time course data/timing Matlab files for each participant are on OSF, while the Python binary classification script and the script with the ANOVA on decoding accuracies are available on a GitHub repo linked in the paper. Altogether, I anticipate it will be relatively straightforward to test reproducibility, but one potential challenge I foresee is making sure the scripts can run properly (i.e., configuring an environment with the appropriate dependencies), though I do see a link to a `.yml` file for a conda environment, so I am optimistic. Given that all classification and ANOVA scripts are available, I also plan to re-implement both analyses from scratch in Python as a training exercise and compare my results to those from the reproducibility attempt and the paper’s reported findings.

***Simulation: Image Similarity & Feature Space***  
I plan to create a new set of 2D stimuli and analyze them using computer vision models like GIST or SimCLR. To create the new silhouette stimuli, I will vary the properties of radial frequency components with custom Matlab scripts (I can refer to the Matlab stimulus-generation scripts created by the authors and hosted on their GitHub repo as a starting point; these materials are from my own lab, and I have received permission to use them). The goal will be to make 16 stimuli that continuously vary along the two different dimensions. I will then run these stimuli through GIST and SimCLR (pre-trained versions of SimCLR models can be downloaded online, and GIST scripts can be found in the authors’ repo). 

Afterward, I will apply principal component analysis (PCA) on the GIST features to visualize and confirm the 2D structure of the stimulus space (this PCA script is also in the repo, but I plan to try and implement this myself as a training exercise). I plan to also calculate the category separability across decision boundaries to see if I can recreate the same patterns in the paper, with higher separability for the linear boundaries compared to the non-linear boundary across both models. Category separability was measured as how different images from separate categories were compared to images from within the same category. More specifically, it was calculated as *(b − w)/(b + w)*, where *b* is the average Euclidean distance between categories and *w* is the average distance within categories, computed from pairwise feature vectors from the GIST and SimCLR models. I will implement these calculations myself in either Python or R.

For this simulation portion, I think a main challenge will be figuring out how to create the new stimuli – I am not familiar with generating novel stimuli that fit certain parameters of dimensionality. Moreover, although the computer vision models used here are already implemented, I do not have first-hand experience using them.

**Links**

- [GitHub Repo](https://github.com/Lena-Kemmelmeier/henderson2025)
- [Original Paper folder](https://github.com/Lena-Kemmelmeier/henderson2025/tree/main/original_paper)
- [Original Paper (PDF)](https://github.com/Lena-Kemmelmeier/henderson2025/blob/main/original_paper/henderson_2025.pdf){target=_blank}


## Methods

### Reliability and Validity
The authors did not report any metrics relating to the reliability of their data (I added this section as part of our in class exercise).

### Power Analysis
A post-hoc power analysis showed that the current sample (N = 10) provided 76% power to detect the observed two-way interaction (ηₚ² = .50, Cohen’s f = 0.999) at α = .05. To achieve 80%, 90% and 95% power, the study would have needed to collect sample sizes of approximately 11, 14, and 16, respectively.

### Sample
Ten participants (7 female) underwent three fMRI sessions, during which they completed a shape categorization task. Participants were recruited from the UCSD community, and were between 24 and 33 (M: 28.2, SD: 3.0) years old. All participants met inclusion criteria of having normal or corrected-to-normal vision.

### Materials

**Shape Stimuli**
The below section is adapted from Henderson et al. (see quoted excerpt below). I will follow the same overall procedure for constructing a two-dimensional shape space but will vary a different pair of radial frequency components (RFCs) than those used in the original study. Each RFC defines a sinusoidal modulation of a base circle’s radius as a function of angle, and varying its amplitude changes the prominence of that modulation in the contour. As in the original design, the amplitudes of two RFCs will be systematically varied while the remaining RFCs are held constant. These two amplitude values define the x and y coordinates of a continuous two-dimensional shape space, where each coordinate pair corresponds to a unique silhouette.

The manipulation of RFC amplitude defines an x/y grid that spans positions between 0 and 5 arbitrary units, representing relative changes in modulation strength. In contrast to the original 4x4 grid of 16 stimuli with coordinates [0.1, 1.7, 3.3, 4.9], I will generate a 6x6 grid of 36 stimuli covering the same range. Grid positions will be evenly spaced and slightly inset from the edges ([0.1, 1.06, 2.02, 2.98, 3.94, 4.9]). This denser grid sampled from the shape space leads to smoother transitions between neighboring shapes. The overall coordinate system (0–5 range with a midpoint at 2.5) will be the same.

"We used a set of shape silhouette stimuli that varied parametrically along two continuous dimensions, generating a 2-dimensional shape space (Fig. 1A). Each shape in this space was a closed contour composed of RFCs29,30. Each shape was composed of 7 different RFCs, where each component has a frequency, amplitude, and phase. We selected these stimuli because they can be represented in a low-dimensional grid-like coordinate system, but are more complex and abstract relative to simpler stimuli such as oriented gratings. Importantly, the changes along each axis in the shape space involve variability in multiple regions of the image, so categorizing the shapes correctly required participants to integrate information globally across the image, rather than focusing on a single part of the shape. For example, to categorize the shapes along axis 1 (Fig. 1A), it might be necessary to integrate information about both the size of the top left lobe, and the shape of the protrusion on the right side. Thus, it would not be possible to categorize the shapes by attending to one focal spatial location only.

To generate the 2-dimensional shape space, we parametrically varied the amplitude of two RFCs, leaving the others constant. The manipulation of RFC amplitude was used to define an x/y grid in arbitrary units that spanned positions between 0 and 5 arb. units, with adjacent grid positions spaced by 0.1 arb. units. All shape space positions on all trials were sampled from this grid of shape space positions. We also defined a coarser grid of 16 points (a 4 × 4 grid) which was used to generate the 16 stimuli that were shown on the majority of trials; this grid is referred to as the “main grid”, and included all x/y combinations of the points [0.1, 1.7, 3.3, 4.9] in shape space coordinates. Stimuli corresponding to points in shape space that were not part of the main grid were used to make the tasks more difficult, see Main task design for details.

We divided the shape space into four quadrants by imposing boundaries at the center position of the grid (2.5 arb. units) in each dimension. To define the binary categories that were relevant for each task (see Main task design), we grouped together two quadrants at a time, with the Linear-1 task and Linear-2 tasks grouping quadrants that were adjacent (creating either a vertical or horizontal linear boundary in shape space), and the Nonlinear task grouping quadrants that were non-adjacent (creating a non-linear boundary). During task training as well as before each scanning run, we utilized a “prototype” image for each shape space quadrant as a way of reminding participants of the current categorization rule. The prototype for each quadrant was positioned directly in the middle of the four main grid positions corresponding to that quadrant (i.e., the x/y coordinates for the prototypes were combinations of [0.9, 4.1] arb. units). These prototype images were never shown during the categorization task trials, to prevent participants from simply memorizing the prototypes. Shapes used in the task were also never positioned exactly on any quadrant boundary in order to prevent any ambiguity about category."


### Procedure	

**Main Task**
This text below is directly quoted from Henderson et. al, and describes the shape categorization task with the three different decision boundary manipulations as well as the general procedure of the experiment. I will not be reproducing/simulating behavioral results so this was 'followed precisely.''

"The main experimental task consisted of categorizing shape silhouette stimuli (Fig. 1) into binary categories. There were three task conditions: Linear-1, Linear-2, and Nonlinear, each of which corresponded to a different binary categorization rule. Shape stimuli were drawn from a two-dimensional shape space coordinate system (see Shape stimuli). The Linear-1 and Linear-2 tasks used a boundary that was linear in this shape space, while the Nonlinear task used a boundary that was non-linear in this shape space (requiring participants to group non-adjacent quadrants into a single category, see Fig. 1 for illustration). Each trial consisted of the presentation of one shape for 1 s, and trials were separated by an inter-trial interval (ITI) that was variable in length, uniformly sampled from the interval 1–5 s. Participants responded on each trial with a button press (right index or middle finger) to indicate which binary category the currently viewed shape fell into; the mapping between category and response was counter-balanced within each scanning session. Participants were allowed to make a response anytime within the window of 2 s from stimulus onset. Feedback was given at the end of each run, and included the participant’s overall accuracy, as well as their accuracy broken down into “easy” and “hard” trials (see next paragraph for description of hard trials), and the number of trials on which they failed to respond. No feedback was given after individual trials.

Each run in the task consisted of 48 trials and lasted 261 s (327 TRs). Of the 48 trials, 32 of these used shapes that were sampled from a grid of 16 points evenly spaced within shape space (“main grid”, see Shape stimuli), each repeated twice. These 16 shapes were presented twice per run regardless of task condition. The remaining 16 trials (referred to as “hard” trials) used shapes that were variable depending on the current task condition and the difficulty level set by the experimenter. The purpose of these trials was to allow the difficulty level to be controlled by the experimenter so that task accuracy could be equalized across all task conditions, and prevent any single task from being trivially easy for each participant. For each run of each task, the experimenter selected a difficulty level between 1 and 13, with each level corresponding to a particular bin of distances from the active categorization boundary (higher difficulty denotes closer distance to boundary). These difficulty levels were adjusted on each run during the session by the experimenter, based on performance on the previous run, with the goal of keeping the participant accuracy values within a stable range for all tasks (target range was around 80% accuracy). For the Nonlinear task, the distance was computed as a linear distance to the nearest boundary. The “hard” trials were generated by randomly sampling 16 shapes from the specified distance bin, with the constraint that 4 of the shapes had to come from each of the four quadrants in shape space. This manipulation ensured that responses were balanced across categories within each run. For many of the analyses presented here, we excluded these hard trials, focusing only on the “main grid” trials where the same images were shown across all task conditions.

Participants performed 12 runs of the main task within each scanning session, for a total of 36 runs across all 3 sessions (with the exception of one participant (S06) for whom 3 runs are missing due to a technical error). The 12 runs in each session were divided into 6 total “parts” where each part consisted of a pair of 2 runs having the same task condition and the same response mapping (3 conditions × 2 response mappings = 6 parts). Each part was preceded by a short training run, which consisted of 5 trials, each trial consisting of a shape drawn from the main grid. The scanner was not on during these training runs, and the purpose of these was to remind the participant of both the currently active task and the response mapping before they began performing the task runs for that part. The order in which the 6 parts were shown was counter-balanced across sessions. Before each scan run began, the participant was again reminded of the current task and response mapping via a display that presented four prototype shapes, one for each shape space quadrant (see Shape stimuli for details on prototype shapes). The prototypes were arranged with two to the left of fixation and two to the right of fixation, and the participant was instructed that the two leftmost shapes corresponded to the index finger button and the two rightmost shapes corresponded to the middle finger button. This display of prototype shapes was also used during the training runs to provide feedback after each trial: after each training trial, the four prototype shapes were shown, and the two prototypes corresponding to the correct category were outlined in green, with accompanying text that indicated whether the participant’s response was correct or incorrect. This feedback display was not shown during the actual task runs.

Before the scan sessions began, participants were trained to perform the shape categorization tasks in a separate behavioral session (training session took place on average 4.0 days before the first scan session). During this behavioral training session, participants performed the same task that they performed in the scanner, including 12 main task runs (2 runs for each combination of condition and response mapping; i.e., each of the 6 parts). As in the scan sessions, each part was preceded by training runs that consisted of 5 trials, each accompanied by feedback. Participants completed between 1 and 3 training runs before starting each part. Average training session accuracy was 0.81 ± 0.02 (mean ± SEM across 10 participants) for the Linear-1 task, 0.81 ± 0.02 for the Linear-2 task, and 0.78 ± 0.02 for the Nonlinear task."


### Analysis Plan

**Binary Classifier Analysis**
**I plan to assess whether I can reproduce the reported Task × Boundary interaction effect on decoding accuracy from the ANOVA that included ROI, Task, and Boundary on voxel activation data.** My dependent variable will be classifier accuracy computed for each participant across ROI, Task, Boundary, and Distance (near vs. far). I will implement the decoding pipeline from scratch in Python, using the preprocessed voxel time course data available on OSF rather than the raw fMRI files, get these classifier accuracies.

Specifically (in like with Henderson et. al), I will train a binary logistic regression classifier to predict category membership (Category A vs. Category B) from voxel activation patterns within each ROI. Classification will be run separately for each task and boundary type using a leave-one-run-out cross-validation setup. I will then run a repeated-measures ANOVA on mean decoding accuracy with factors ROI, Task, Boundary, and Distance, focusing on the Task × Boundary interaction for near trials.

As a visual check, I will plot the overall decoding accuracy patterns shown in Figure 2A–C to confirm that the pattern of results matches the original paper (I understand the exact means may shift). Specifically, I expect to see higher accuracy for the Linear tasks than the Nonlinear task, with a similar pattern of accuracies (and significance) across ROIs. I will consider the replication successful if I can reproduce both this overall pattern of classifier accuracy and the significant Task × Boundary interaction (Linear-2 > Linear-1 for near trials).

**Image Similarity & Feature Space Analyses**
For my simulation portion of this project, I plan to replicate this section exactly (see quoted section below), using the SimCLR and GIST scripts from the authors’ GitHub repository. I will also extract features from each of my newly generated stimuli and run a simple PCA to check that the feature space captures a clear two-dimensional structure, visualized separately for each shape dimension to make sure the model representations change smoothly along both axes of the shape space (like Figure 1C in the paper). Moreover, I anticipate seeing similar results as Figure 1D, where category separability is highest for the linear boundaries and lowest for the nonlinear boundary across both models, with overall separability values being slightly higher for GIST compared to SimCLR. Again, category separability will be measured as how different images from separate categories were compared to images from within the same category. More specifically, as *(b − w)/(b + w)*, where *b* is the average Euclidean distance between categories and *w* is the average distance within categories, computed from pairwise feature vectors from the GIST and SimCLR models.

"To estimate the perceptual discriminability of our shape categories, we used two computer vision models to extract activations in response to each stimulus image. We first used the GIST model31, which is based on Gabor filters and captures low-level spectral image properties. We also extracted features from a pre-trained SimCLR model32, which is a self-supervised model trained using contrastive learning on a large image database. We selected these two models because the GIST model captures clearly defined image properties similar to those represented in the early visual system, while the SimCLR model can capture a wider set of image features, including mid-level and high-level properties. The GIST model was implemented in Matlab, using a 4 × 4 spatial grid, 4 spatial scales, and 4 orientations per spatial scale. The version of SimCLR that we used was implemented in PyTorch and used a ResNet-50 backbone (pre-trained model downloaded from https://pypi.org/project/simclr/). We extracted activations from blocks 2, 6, 12, and 15, and performed a max-pooling operation (kernel size = 4, stride = 4) to reduce the size of activations from each block. We used principal components analysis (PCA) to further reduce the size of activations, retaining a maximum of 500 components per block, and concatenated the resulting features across all blocks.

Using these activations, we computed the separability of shape categories across each of our boundaries (Linear-1, Linear-2, Nonlinear) by computing all pairwise Euclidean distances between main grid shapes in the same category (within-category distances) and main grid shapes in different categories (between-category distances). We then computed the average of the within-category distances (w) and between-category distances (b). The separability measure for each boundary was computed as: (b−w)/(b+w)."

This simulation analysis is meant to complement the fMRI decoding results by testing whether similar patterns show up just from the visual features of the stimuli themselves. If GIST and SimCLR both show higher separability for the linear boundaries than for the nonlinear boundary, that would suggest that part of the effect seen in visual cortex could be explained by the image properties alone. But if that same pattern doesn’t appear in the feature-space models, it would suggest that task demands, not just low-level visual differences, shape how these stimuli are represented in the brain.

### Differences from Original Study

I will use the preprocessed voxel activation data they shared instead of reanalyzing the raw fMRI data, and am only reproducing the decoding and ANOVA portions rather than the full fMRI acquisition. I plan to rebuild the binary decoding and ANOVA pipeline in Python as a training exercise. I will not be reproducing the behavioral data analysis.

For the simulation part, I will make a new set of two-dimensional shape stimuli that follow the same general structure as the original ones but vary a different pair of radial frequency components. Instead of the original 4x4 grid, I will use a denser 6x6 grid so the changes between neighboring shapes are smoother. I will then run these new shapes through the GIST and SimCLR models to see how separable the categories are across different boundaries. The goal is not to reproduce participants’ neural or behavioral responses, but to see whether similar structure shows up just from the visual features of the images, while also gaining practice generating and analyzing new stimuli.

## Progress Check 2 (Reproducibility)
**Again, I am assessing whether I can reproduce the reported Task × Boundary interaction effect on decoding accuracy from the three-way ANOVA that included ROI, Task, and Boundary on voxel activation data.** My dependent variable is classifier accuracy computed for each participant across ROI, Task, and Boundary conditions focusing on the 'near' trials (trials where the shape to be categorized was close to the decision boundary). 

So far, I've implemented the binary decoding pipeline from scratch in Python following their leave-one-run-out cross-validation setup. I use the preprocessed data available on OSF rather than raw fMRI files (which are not available) to obtain these classifier accuracies. It looks like the decoding pipeline works when I examine a few participants, but it does take a bit of time to run all ten, so I plan to let it run on one of our lab's remote neurocomputing servers. The steps I've taken seem to align with what they describe in-text.

I did try to implement the loading/conversion of the fMRI data completely from scratch, but ended up pivoting to using the authors' helper functions in converting everything from .mat to .npz because it was a huge headache (they said themselves that their method was a bit 'hacky' which sort of hints at how much of a pain it was). Included in their helper functions was also implementation to z-score and mean-center the BOLD data, which I took advantage of. If I have more time later, I plan to go back and troubleshoot my original pipeline to see if I can instead code these parts myself. I think a barrier here is that I am a bit unfamiliar with the way fMRI files are structures -- the mat files have so many layers and fields to keep track of.

I have the implementation of the ANOVA done so I'll be ready to run it whenever the decoder is done running. I will consider my replication successful if I also find a significant Task × Boundary interaction effect on decoding accuracy (such that Linear-2 > Linear-1 for near trials), as well as a similar effect size (the authors did not report an effect size, but I am computing this post-hoc and hope to find a similar magnitude)

Additionally, as a visual check, I will plot the overall decoding accuracy patterns shown in Figure 2A–C to confirm that my pattern of results is simlar to the original paper. To make this comparison a bit smoother, I also imported their plotting helpers so that I can check my ability to reproduce their exact Figure 2A-C. I expect that the means may shift, but hope the general pattern will hold. To reiterate, I anticipate higher accuracy for the Linear tasks than the Nonlinear task, with a similar pattern of accuracies (and significance) across ROIs.

## Progress Check 3 (Continuing Reproducibility + Beginning Simulation)
As an update from the last progress check: I tried to re-implement the authors’ conversion of the voxel data from .mat files into the h5py format, but it was still a headache. I do have a MATLAB script that successfully compiles the data in a Python-friendly way, but I have not finished editing my decoding script to remove the calls to the authors’ helper functions. For now, I’m still using the version that relies on those helpers to keep momentum. I plan to revisit this before the final deadline to see if I can fully re-implement the pipeline, though it’s a lower priority because I feel like it isn't the most valuable experience for my training.

**I finished training the binary decoders, and re-created Figures 2A-C from Henderson et al. (see Figures 1-3 below). The pattern of data closely match theirs!** One thing I did edit from the original paper is that I raised the number of max iterations in the logistic regression cross-validation to 20,000, and set the tolerance to 1e-3. Before I adjusted these params, I was getting convergence warnings from sklearn. I know these are warnings, and not errors, but it was just a preference on my end (felt more comfortably knowing the model converged on a solution/reached a stable point). 

::: {layout-ncol=3}
::: {layout-ncol=3}
![Binary classifier: Predict "Linear-1" category.](../bold_decoding_anova_results/figs/Figure2_linear1_replica_DOTSEM.png){#fig-2A fig-cap="Binary classifier predicting Linear-1 category." fig-alt="Scatter plot of classifier accuracy across ROIs for Linear-1 boundary." width="100%" fig-align="center"}

![Binary classifier: Predict "Linear-2" category.](../bold_decoding_anova_results/figs/Figure2_linear2_replica_DOTSEM.png){#fig-2B fig-cap="Binary classifier predicting Linear-2 category." fig-alt="Scatter plot of classifier accuracy across ROIs for Linear-2 boundary." width="100%" fig-align="center"}

![Binary classifier: Predict "Nonlinear" category.](../bold_decoding_anova_results/figs/Figure2_nonlinear_replica_DOTSEM.png){#fig-2C fig-cap="Binary classifier predicting Nonlinear category." fig-alt="Scatter plot of classifier accuracy across ROIs for Nonlinear boundary." width="100%" fig-align="center"}
:::

I also ran the 3-way ANOVA (ROI × Task × Boundary) on the classification accuracy from the binary decoder above (includes data for near-trials only, labels: Linear1 vs. Linear2). My results all seem very off from the authors' but I realized that is probably a mistake on my end with wrangling my data, so I plan to troubleshoot this! After I get the ANOVA result, my reproducibility portion will be officially done.

I also made progress on the simulation portion of my experience (creating a new batch of stimuli). Specifically, I created the 36 images that are a part of the 6x6 array I described above (see Figure 4 below, where I arranged them all in the grid). The authors had about a dozen different scripts in Matlab that they used for stimulus generation. I re-purposed/implemented the legacy functions they used (giving proper credit), and basically re-consolidated all the functionality to one script. I varied the amplitude of 2 RFCs different from the ones the authors used. I also chose to use a different "shape family." My next step will be running the images through the computer vision models, then calculating category separability and running PCA.

![6×6 RFC blob grid.](../stimuli/new_blob_stim/grid_montage.png){#fig-blobgrid fig-cap="Blob grid (6×6) spanning 0–5 RFC amplitudes on each axis." fig-alt="36 grayscale blob silhouettes arranged in a 6×6 grid." width="50%" fig-align="center"}

**Link to Preregistration** [OSF Link](https://doi.org/10.17605/OSF.IO/M7FYH)

-----
As a side note, I've doing most of my Python coding (especially for the classification) within .py files in the Spyder IDE. I do have a .qmd version of the classification pipeline (understand this is more in line with the class objectives, especially because Quarto/Pixi lend themselves to reproducibility), but it's been MUCH easier for me to crash-test my code in Spyder's IDE. Moreover, I ran the models on one of the external neurocomputing servers in a .py file because it was easiest for me. Once everything is finalized, I'll update/shift any scripts over to be .qmd.

## "Ready to Run" Progress Check
I de-bugged my code for reproducing the ANOVA Task x Boundary interaction -- it turns out there was an issue with how I masked near vs. far trials (only near trials were included in this analysis). I would say I successfully reproduced their result, as the effect size of this interaction was close to the one in the original study, ηₚ² = .50, and because this effect was significant (see Figure 5). 

![Three-way ANOVA that included ROI, Task, and Boundary on voxel activation data for items near the decision boundary.](../bold_decoding_anova_results/figs/Table1_LINEAR_NEARONLY_replica_APA.png){#fig-blobgrid fig-cap="" fig-alt="Three-way ANOVA that included ROI, Task, and Boundary on voxel activation data for items near the decision boundary." width="100%" fig-align="center"}

## Results

Note from Bria for the results: Plan to include a side-by-side panel showing your replicated accuracy pattern next to the original Task × Boundary figure, with effect sizes and confidence intervals.

For image similarity (another note from Bria): For visualization, I would plan a simple PCA/tSNE plot of model features, annotated by boundary type.


### Data preparation

Data preparation following the analysis plan.
	
```{r include=F}
### Data Preparation

#### Load Relevant Libraries and Functions

#### Import data

#### Data exclusion / filtering

#### Prepare data for analysis - create columns etc.
```

### Confirmatory analysis

The analyses as specified in the analysis plan.  

*Side-by-side graph with original graph is ideal here*

### Exploratory analyses

Any follow-up analyses desired (not required).  

## Discussion

### Summary of Replication Attempt

Open the discussion section with a paragraph summarizing the primary result from the confirmatory analysis and the assessment of whether it replicated, partially replicated, or failed to replicate the original result.  

### Commentary

Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis, (b) assessment of the meaning of the replication (or not) - e.g., for a failure to replicate, are the differences between original and present study ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the replication attempt.  None of these need to be long.
